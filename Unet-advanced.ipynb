{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import tifffile as tiff\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_Cls = 10\n",
    "inDir = '../input'\n",
    "DF = pd.read_csv(inDir + '/train_wkt_v4.csv')\n",
    "GS = pd.read_csv(inDir + '/grid_sizes.csv', names=['ImageId', 'Xmax', 'Ymin'], skiprows=1)\n",
    "SB = pd.read_csv(os.path.join(inDir, 'sample_submission.csv'))\n",
    "ISZ = 160\n",
    "smooth = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_coordinates_to_raster(coords, img_size, xymax):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    Xmax, Ymax = xymax\n",
    "    H, W = img_size\n",
    "    W1 = 1.0 * W * W / (W + 1)\n",
    "    H1 = 1.0 * H * H / (H + 1)\n",
    "    xf = W1 / Xmax\n",
    "    yf = H1 / Ymax\n",
    "    coords[:, 1] *= yf\n",
    "    coords[:, 0] *= xf\n",
    "    coords_int = np.round(coords).astype(np.int32)\n",
    "    return coords_int\n",
    "\n",
    "\n",
    "def _get_xmax_ymin(grid_sizes_panda, imageId):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    xmax, ymin = grid_sizes_panda[grid_sizes_panda.ImageId == imageId].iloc[0, 1:].astype(float)\n",
    "    return (xmax, ymin)\n",
    "\n",
    "\n",
    "def _get_polygon_list(wkt_list_pandas, imageId, cType):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    df_image = wkt_list_pandas[wkt_list_pandas.ImageId == imageId]\n",
    "    multipoly_def = df_image[df_image.ClassType == cType].MultipolygonWKT\n",
    "    polygonList = None\n",
    "    if len(multipoly_def) > 0:\n",
    "        assert len(multipoly_def) == 1\n",
    "        polygonList = wkt_loads(multipoly_def.values[0])\n",
    "    return polygonList\n",
    "\n",
    "\n",
    "def _get_and_convert_contours(polygonList, raster_img_size, xymax):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    perim_list = []\n",
    "    interior_list = []\n",
    "    if polygonList is None:\n",
    "        return None\n",
    "    for k in range(len(polygonList)):\n",
    "        poly = polygonList[k]\n",
    "        perim = np.array(list(poly.exterior.coords))\n",
    "        perim_c = _convert_coordinates_to_raster(perim, raster_img_size, xymax)\n",
    "        perim_list.append(perim_c)\n",
    "        for pi in poly.interiors:\n",
    "            interior = np.array(list(pi.coords))\n",
    "            interior_c = _convert_coordinates_to_raster(interior, raster_img_size, xymax)\n",
    "            interior_list.append(interior_c)\n",
    "    return perim_list, interior_list\n",
    "\n",
    "\n",
    "def _plot_mask_from_contours(raster_img_size, contours, class_value=1):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    img_mask = np.zeros(raster_img_size, np.uint8)\n",
    "    if contours is None:\n",
    "        return img_mask\n",
    "    perim_list, interior_list = contours\n",
    "    cv2.fillPoly(img_mask, perim_list, class_value)\n",
    "    cv2.fillPoly(img_mask, interior_list, 0)\n",
    "    return img_mask\n",
    "\n",
    "\n",
    "def generate_mask_for_image_and_class(raster_size, imageId, class_type, grid_sizes_panda=GS, wkt_list_pandas=DF):\n",
    "    # __author__ = visoft\n",
    "    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n",
    "    xymax = _get_xmax_ymin(grid_sizes_panda, imageId)\n",
    "    polygon_list = _get_polygon_list(wkt_list_pandas, imageId, class_type)\n",
    "    contours = _get_and_convert_contours(polygon_list, raster_size, xymax)\n",
    "    mask = _plot_mask_from_contours(raster_size, contours, 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def M(image_id):\n",
    "    # __author__ = amaia\n",
    "    # https://www.kaggle.com/aamaia/dstl-satellite-imagery-feature-detection/rgb-using-m-bands-example\n",
    "    img = tiff.imread('../input/sixteen_band/{}_M.tif'.format(image_id))\n",
    "    img = np.rollaxis(img, 0, 3)\n",
    "    return img\n",
    "\n",
    "\n",
    "def stretch_n(bands, lower_percent=0, higher_percent=100):\n",
    "    out = np.zeros_like(bands, dtype=np.float32)\n",
    "    n = bands.shape[2]\n",
    "    for i in range(n):\n",
    "        a = 0  # np.min(band)\n",
    "        b = 1  # np.max(band)\n",
    "        c = np.percentile(bands[:, :, i], lower_percent)\n",
    "        d = np.percentile(bands[:, :, i], higher_percent)\n",
    "        t = a + (bands[:, :, i] - c) * (b - a) / (d - c)\n",
    "        t[t < a] = a\n",
    "        t[t > b] = b\n",
    "        out[:, :, i] = t\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def stick_all_train():\n",
    "    print(\"let's stick all imgs together\")\n",
    "    s = 835\n",
    "\n",
    "    x = np.zeros((5 * s, 5 * s, 8))\n",
    "    y = np.zeros((5 * s, 5 * s, N_Cls))\n",
    "\n",
    "    ids = sorted(DF.ImageId.unique())\n",
    "    print(len(ids))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            id = ids[5 * i + j]\n",
    "\n",
    "            img = M(id)\n",
    "            img = stretch_n(img)\n",
    "            print(img.shape, id, np.amax(img), np.amin(img))\n",
    "            x[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n",
    "            for z in range(N_Cls):\n",
    "                y[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class(\n",
    "                    (img.shape[0], img.shape[1]), id, z + 1)[:s, :s]\n",
    "\n",
    "    print(np.amax(y), np.amin(y))\n",
    "\n",
    "    np.save('../input/data/x_trn_%d' % N_Cls, x)\n",
    "    np.save('../input/data/y_trn_%d' % N_Cls, y)\n",
    "\n",
    "\n",
    "def get_patches(img, msk, amt=10000, aug=True):\n",
    "    is2 = int(1.0 * ISZ)\n",
    "    xm, ym = img.shape[0] - is2, img.shape[1] - is2\n",
    "\n",
    "    x, y = [], []\n",
    "\n",
    "    tr = [0.4, 0.1, 0.1, 0.15, 0.3, 0.95, 0.1, 0.05, 0.001, 0.005]\n",
    "    for i in range(amt):\n",
    "        xc = random.randint(0, xm)\n",
    "        yc = random.randint(0, ym)\n",
    "\n",
    "        im = img[xc:xc + is2, yc:yc + is2]\n",
    "        ms = msk[xc:xc + is2, yc:yc + is2]\n",
    "\n",
    "        for j in range(N_Cls):\n",
    "            sm = np.sum(ms[:, :, j])\n",
    "            if 1.0 * sm / is2 ** 2 > tr[j]:\n",
    "                if aug:\n",
    "                    if random.uniform(0, 1) > 0.5:\n",
    "                        im = im[::-1]\n",
    "                        ms = ms[::-1]\n",
    "                    if random.uniform(0, 1) > 0.5:\n",
    "                        im = im[:, ::-1]\n",
    "                        ms = ms[:, ::-1]\n",
    "\n",
    "                x.append(im)\n",
    "                y.append(ms)\n",
    "\n",
    "    x, y = 2 * np.transpose(x, (0, 3, 1, 2)) - 1, np.transpose(y, (0, 3, 1, 2))\n",
    "    print(x.shape, y.shape, np.amax(x), np.amin(x), np.amax(y), np.amin(y))\n",
    "    return x, y\n",
    "\n",
    "def make_val():\n",
    "    print(\"let's pick some samples for validation\")\n",
    "    img = np.load('../input/data/x_trn_%d.npy' % N_Cls)\n",
    "    msk = np.load('../input/data/y_trn_%d.npy' % N_Cls)\n",
    "    x, y = get_patches(img, msk, amt=3000)\n",
    "\n",
    "    np.save('../input/data/x_tmp_%d' % N_Cls, x)\n",
    "    np.save('../input/data/y_tmp_%d' % N_Cls, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((8, ISZ, ISZ))\n",
    "    conv1 = Conv2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Conv2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Conv2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Conv2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Conv2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Conv2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Conv2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Conv2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Conv2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Conv2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Conv2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Conv2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Conv2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Conv2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(N_Cls, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
    "    return model\n",
    "\n",
    "# def get_unet():\n",
    "#     inputs = Input((8, ISZ, ISZ))\n",
    "#     conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "#     conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "#     conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "#     conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "#     conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "#     conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "#     conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "#     conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "#     pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "#     conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "#     conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "#     up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "#     conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "#     conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "#     up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "#     conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "#     conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "#     up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "#     conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "#     conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "#     up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "#     conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "#     conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "#     conv10 = Convolution2D(N_Cls, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "#     model = Model(input=inputs, output=conv10)\n",
    "#     model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_jacc(model):\n",
    "    img = np.load('../input/data/x_tmp_%d.npy' % N_Cls)\n",
    "    msk = np.load('../input/data/y_tmp_%d.npy' % N_Cls)\n",
    "\n",
    "    prd = model.predict(img, batch_size=4)\n",
    "    print(prd.shape, msk.shape)\n",
    "    avg, trs = [], []\n",
    "\n",
    "    for i in range(N_Cls):\n",
    "        t_msk = msk[:, i, :, :]\n",
    "        t_prd = prd[:, i, :, :]\n",
    "        t_msk = t_msk.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
    "        t_prd = t_prd.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
    "\n",
    "        m, b_tr = 0, 0\n",
    "        for j in range(10):\n",
    "            tr = j / 10.0\n",
    "            pred_binary_mask = t_prd > tr\n",
    "\n",
    "            jk = jaccard_similarity_score(t_msk, pred_binary_mask)\n",
    "            if jk > m:\n",
    "                m = jk\n",
    "                b_tr = tr\n",
    "        print(i, m, b_tr)\n",
    "        avg.append(m)\n",
    "        trs.append(b_tr)\n",
    "\n",
    "    score = sum(avg) / 10.0\n",
    "    return score, trs\n",
    "\n",
    "\n",
    "def mask_for_polygons(polygons, im_size):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "\n",
    "def mask_to_polygons(mask, epsilon=1, min_area=1.):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    image, contours, hierarchy = cv2.findContours(\n",
    "        ((mask == 1) * 255).astype(np.uint8),\n",
    "        cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    # create approximate contours to have reasonable submission size\n",
    "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                       for cnt in contours]\n",
    "    if not contours:\n",
    "        return MultiPolygon()\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(approx_contours[idx])\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    for idx, cnt in enumerate(approx_contours):\n",
    "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
    "            assert cnt.shape[1] == 1\n",
    "            poly = Polygon(\n",
    "                shell=cnt[:, 0, :],\n",
    "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
    "                       if cv2.contourArea(c) >= min_area])\n",
    "            all_polygons.append(poly)\n",
    "    # approximating polygons might have created invalid ones, fix them\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n",
    "        # need to keep it a Multi throughout\n",
    "        if all_polygons.type == 'Polygon':\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "    return all_polygons\n",
    "\n",
    "\n",
    "def get_scalers(im_size, x_max, y_min):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "    h, w = im_size  # they are flipped so that mask_for_polygons works correctly\n",
    "    h, w = float(h), float(w)\n",
    "    w_ = 1.0 * w * (w / (w + 1))\n",
    "    h_ = 1.0 * h * (h / (h + 1))\n",
    "    return w_ / x_max, h_ / y_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net():\n",
    "    print(\"start train net\")\n",
    "    x_val, y_val = np.load('../input/data/x_tmp_%d.npy' % N_Cls), np.load('data/y_tmp_%d.npy' % N_Cls)\n",
    "    img = np.load('../input/data/x_trn_%d.npy' % N_Cls)\n",
    "    msk = np.load('../input/data/y_trn_%d.npy' % N_Cls)\n",
    "\n",
    "    x_trn, y_trn = get_patches(img, msk)\n",
    "\n",
    "    model = get_unet()\n",
    "    model.load_weights('../input/weights/unet_10_jk0.7878')\n",
    "    model_checkpoint = ModelCheckpoint('../input/weights/unet_tmp.hdf5', monitor='loss', save_best_only=True)\n",
    "    for i in range(1):\n",
    "        model.fit(x_trn, y_trn, batch_size=64, nb_epoch=1, verbose=1, shuffle=True,\n",
    "                  callbacks=[model_checkpoint], validation_data=(x_val, y_val))\n",
    "        del x_trn\n",
    "        del y_trn\n",
    "        x_trn, y_trn = get_patches(img, msk)\n",
    "        score, trs = calc_jacc(model)\n",
    "        print('val jk', score)\n",
    "        model.save_weights('../input/weights/unet_10_jk%.4f' % score)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_id(id, model, trs):\n",
    "    img = M(id)\n",
    "    x = stretch_n(img)\n",
    "\n",
    "    cnv = np.zeros((960, 960, 8)).astype(np.float32)\n",
    "    prd = np.zeros((N_Cls, 960, 960)).astype(np.float32)\n",
    "    cnv[:img.shape[0], :img.shape[1], :] = x\n",
    "\n",
    "    for i in range(0, 6):\n",
    "        line = []\n",
    "        for j in range(0, 6):\n",
    "            line.append(cnv[i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ])\n",
    "\n",
    "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
    "        tmp = model.predict(x, batch_size=4)\n",
    "        for j in range(tmp.shape[0]):\n",
    "            prd[:, i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ] = tmp[j]\n",
    "\n",
    "    trs = [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1]\n",
    "    for i in range(N_Cls):\n",
    "        prd[i] = prd[i] > trs[i]\n",
    "\n",
    "    return prd[:, :img.shape[0], :img.shape[1]]\n",
    "\n",
    "\n",
    "def predict_test(model, trs = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]):\n",
    "    print(\"predict test\")\n",
    "    for i, id in enumerate(sorted(set(SB['ImageId'].tolist()))):\n",
    "        msk = predict_id(id, model, trs)\n",
    "        np.save('../output/msk/10_%s' % id, msk)\n",
    "        if i % 100 == 0: print(i, id)\n",
    "\n",
    "\n",
    "def make_submit():\n",
    "    print(\"make submission file\")\n",
    "    df = pd.read_csv('../input/sample_submission.csv')\n",
    "    print(df.head())\n",
    "    for idx, row in df.iterrows():\n",
    "        id = row[0]\n",
    "        kls = row[1] - 1\n",
    "\n",
    "        msk = np.load('../output/msk/10_%s.npy' % id)[kls]\n",
    "        pred_polygons = mask_to_polygons(msk)\n",
    "        x_max = GS.loc[GS['ImageId'] == id, 'Xmax'].as_matrix()[0]\n",
    "        y_min = GS.loc[GS['ImageId'] == id, 'Ymin'].as_matrix()[0]\n",
    "\n",
    "        x_scaler, y_scaler = get_scalers(msk.shape, x_max, y_min)\n",
    "\n",
    "        scaled_pred_polygons = shapely.affinity.scale(pred_polygons, xfact=1.0 / x_scaler, yfact=1.0 / y_scaler,\n",
    "                                                      origin=(0, 0, 0))\n",
    "\n",
    "        df.iloc[idx, 2] = shapely.wkt.dumps(scaled_pred_polygons)\n",
    "        if idx % 100 == 0: print(idx)\n",
    "    print(df.head())\n",
    "    df.to_csv('../output/subm/1.csv', index=False)\n",
    "\n",
    "\n",
    "def check_predict(id='6120_2_3'):\n",
    "    model = get_unet()\n",
    "    model.load_weights('../input/weights/unet_10_jk0.7878')\n",
    "\n",
    "    msk = predict_id(id, model, [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1])\n",
    "    img = M(id)\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.set_title('image ID:6120_2_3')\n",
    "    ax1.imshow(img[:, :, 5], cmap=plt.get_cmap('gist_ncar'))\n",
    "    ax2 = plt.subplot(132)\n",
    "    ax2.set_title('predict bldg pixels')\n",
    "    ax2.imshow(msk[0], cmap=plt.get_cmap('gray'))\n",
    "    ax3 = plt.subplot(133)\n",
    "    ax3.set_title('predict bldg polygones')\n",
    "    ax3.imshow(mask_for_polygons(mask_to_polygons(msk[0], epsilon=1), img.shape[:2]), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `InputLayer` call to the Keras 2 API: `InputLayer(batch_input_shape=[None, 8, ..., sparse=False, name=\"input_8\", dtype=\"float32\")`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_134\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=32, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_135\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=32, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_29\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_136\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=64, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_137\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=64, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_30\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_138\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_139\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_31\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_140\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=256, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_141\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=256, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_32\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_142\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=512, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_143\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=512, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_144\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=256, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_145\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=256, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_146\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_147\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_148\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=64, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_149\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=64, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_150\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=32, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_151\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=32, strides=[1, 1], padding=\"same\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_152\", activity_regularizer=None, trainable=True, activation=\"sigmoid\", kernel_size=(1, 1), filters=10, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junfang/anaconda3/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../input/weights/unet_42quality.hdf5', \\\n",
    "                   custom_objects={'jaccard_coef': jaccard_coef, 'jaccard_coef_int': jaccard_coef_int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict test\n",
      "0 6010_0_0\n",
      "100 6050_1_0\n",
      "200 6090_1_1\n",
      "300 6130_2_4\n",
      "400 6170_4_0\n",
      "make submission file\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'sample_submission.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d3c24fc028a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredict_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmake_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a0c963ffa72d>\u001b[0m in \u001b[0;36mmake_submit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"make submission file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'sample_submission.csv' does not exist"
     ]
    }
   ],
   "source": [
    "predict_test(model)\n",
    "make_submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submit():\n",
    "    print(\"make submission file\")\n",
    "    df = pd.read_csv('../input/sample_submission.csv')\n",
    "    print(df.head())\n",
    "    for idx, row in df.iterrows():\n",
    "        id = row[0]\n",
    "        kls = row[1] - 1\n",
    "\n",
    "        msk = np.load('../output/msk/10_%s.npy' % id)[kls]\n",
    "        pred_polygons = mask_to_polygons(msk)\n",
    "        x_max = GS.loc[GS['ImageId'] == id, 'Xmax'].as_matrix()[0]\n",
    "        y_min = GS.loc[GS['ImageId'] == id, 'Ymin'].as_matrix()[0]\n",
    "\n",
    "        x_scaler, y_scaler = get_scalers(msk.shape, x_max, y_min)\n",
    "\n",
    "        scaled_pred_polygons = shapely.affinity.scale(pred_polygons, xfact=1.0 / x_scaler, yfact=1.0 / y_scaler,\n",
    "                                                      origin=(0, 0, 0))\n",
    "\n",
    "        df.iloc[idx, 2] = shapely.wkt.dumps(scaled_pred_polygons)\n",
    "        if idx % 100 == 0: print(idx)\n",
    "    print(df.head())\n",
    "    df.to_csv('../output/subm/1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make submission file\n",
      "    ImageId  ClassType                                    MultipolygonWKT\n",
      "0  6120_2_4          1  POLYGON ((0 0, 0.009188 0, 0.009188 -0.0090399...\n",
      "1  6120_2_4          2  POLYGON ((0 0, 0.009188 0, 0.009188 -0.0090399...\n",
      "2  6120_2_4          3  POLYGON ((0 0, 0.009188 0, 0.009188 -0.0090399...\n",
      "3  6120_2_4          4  POLYGON ((0 0, 0.009188 0, 0.009188 -0.0090399...\n",
      "4  6120_2_4          5  POLYGON ((0 0, 0.009188 0, 0.009188 -0.0090399...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "    ImageId  ClassType                                    MultipolygonWKT\n",
      "0  6120_2_4          1  MULTIPOLYGON (((0.0089743035872592 -0.00339540...\n",
      "1  6120_2_4          2  MULTIPOLYGON (((0.0090609072406947 -0.00092995...\n",
      "2  6120_2_4          3  MULTIPOLYGON (((0.0091475108941303 -0.00134085...\n",
      "3  6120_2_4          4  MULTIPOLYGON (((0.0090933836107331 -0.00578515...\n",
      "4  6120_2_4          5  MULTIPOLYGON (((0.0091691618074892 -0.00195722...\n"
     ]
    }
   ],
   "source": [
    "make_submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../output/subm/1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassType</th>\n",
       "      <th>MultipolygonWKT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((0.0082238314645629 -0.00601182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((0.0090049869598775 -0.00350960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((0.0078766512444230 -0.00567603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((0.0041878614054370 -0.00779912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((0.0000759456731556 -0.00489611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>6</td>\n",
       "      <td>MULTIPOLYGON (((0.0089724388142394 -0.00204727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>7</td>\n",
       "      <td>MULTIPOLYGON (((0.0000000000000000 0.000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>8</td>\n",
       "      <td>MULTIPOLYGON (((0.0090049869598775 -0.00001083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((0.0057610217779458 -0.00378040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>6050_4_4</td>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((0.0042746564604720 -0.00474446...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  ClassType                                    MultipolygonWKT\n",
       "2460  6050_4_4          1  MULTIPOLYGON (((0.0082238314645629 -0.00601182...\n",
       "2461  6050_4_4          2  MULTIPOLYGON (((0.0090049869598775 -0.00350960...\n",
       "2462  6050_4_4          3  MULTIPOLYGON (((0.0078766512444230 -0.00567603...\n",
       "2463  6050_4_4          4  MULTIPOLYGON (((0.0041878614054370 -0.00779912...\n",
       "2464  6050_4_4          5  MULTIPOLYGON (((0.0000759456731556 -0.00489611...\n",
       "2465  6050_4_4          6  MULTIPOLYGON (((0.0089724388142394 -0.00204727...\n",
       "2466  6050_4_4          7  MULTIPOLYGON (((0.0000000000000000 0.000000000...\n",
       "2467  6050_4_4          8  MULTIPOLYGON (((0.0090049869598775 -0.00001083...\n",
       "2468  6050_4_4          9  MULTIPOLYGON (((0.0057610217779458 -0.00378040...\n",
       "2469  6050_4_4         10  MULTIPOLYGON (((0.0042746564604720 -0.00474446..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['ImageId'] == '6050_4_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
